{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384929b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90893ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pickle import load, dump\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool\n",
    "from scripts.image.kmeans import kmeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc25fa4",
   "metadata": {},
   "source": [
    "Let's open and get the data from the pickles file, we are using the embedddings **after tsne**. You can obtain them by running the `<root>/scripts/reduce_dims.py` script.\n",
    "\n",
    "```\n",
    "python ./scripts/reduce_dims -i ./<path_to_clip_embeddings> -i . -k 2 --method tsne\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../reduced-tsne-k=2.pk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cdad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cde35",
   "metadata": {},
   "source": [
    "Let's also add the category to each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2603f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_categories_to_data(data, categories):\n",
    "    data['categories'] = []\n",
    "    for image_paths in tqdm(data['image_paths']):\n",
    "        # e.g.rf100/chess-pieces-mjzgj/train/images/foo.jpg'\n",
    "        dataset_name = Path(image_paths).parts[1]\n",
    "        category = categories.loc[dataset_name].category\n",
    "        data['categories'].append(category)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b624b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.read_csv(\"../metadata/categories.csv\", index_col=0)\n",
    "data = add_categories_to_data(data, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216ce3d",
   "metadata": {},
   "source": [
    "sweet, now we have all the data we need. Let's do some clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01c80c",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Let's define some transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def read_image_and_transform(image_path, size=(224, 224)):\n",
    "    image_path = \"../\" + image_path\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = F.resize(img, size)\n",
    "    img = F.to_tensor(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d76b81",
   "metadata": {},
   "source": [
    "### Cluster per category\n",
    "\n",
    "We first want to get the 100 most representative images per category, thus we will first filter per category and run kmeans with 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reored_by_left_top(x):\n",
    "    left, top = x[:,0].min(), x[:,1].max()\n",
    "    to_compare = torch.tensor([left, top])\n",
    "    indexes = ((to_compare - x) ** 2).sum(dim=-1).argsort()\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluter_per_category(category=\"real world\", num_clusters=100):\n",
    "    filtered_indexes = np.array(data[\"categories\"]) == category\n",
    "    # let's use the pca ones\n",
    "    filtered_x = data['x'][filtered_indexes]\n",
    "    filtered_image_paths = np.array(data['image_paths'])[filtered_indexes]\n",
    "    # do kmeans\n",
    "    means, bins = kmeans(torch.from_numpy(filtered_x), num_clusters=num_clusters, num_iters=100)\n",
    "#     means = reored_by_left_top(means)\n",
    "    # compute distance between means and all points\n",
    "    diffs = (means[:,None,:] - filtered_x[None,...])\n",
    "    diffs = (diffs**2).sum(dim=-1)\n",
    "    indexes = diffs.argmin(axis=1)\n",
    "    # create the grid\n",
    "    image_paths = filtered_image_paths[indexes]\n",
    "    indexes = reored_by_left_top(filtered_x[indexes])\n",
    "    image_paths = image_paths[indexes]\n",
    "\n",
    "    image = show(\n",
    "        make_grid(\n",
    "            list(map(read_image_and_transform, image_paths)),\n",
    "            nrow=25\n",
    "        )\n",
    "    )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc059a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = Path(\"../paper/images/grid/\")\n",
    "\n",
    "for category in categories.category.unique():\n",
    "    num_clusters = 50\n",
    "    if category == \"real world\":\n",
    "        num_clusters = 200\n",
    "    cluter_per_category(category, num_clusters).savefig(dst / f\"{category}.png\", dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9cd7f",
   "metadata": {},
   "source": [
    "## Grid Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cluster_grid(num_clusters, nrow):\n",
    "    # let's use the pca ones\n",
    "    x = data['x']\n",
    "    image_paths = np.array(data['image_paths'])\n",
    "    # do kmeans\n",
    "    means, bins = kmeans(torch.from_numpy(x), num_clusters=num_clusters, num_iters=50)\n",
    "    diffs = (means[:,None,:] - x[None,...])\n",
    "    diffs = (diffs**2).sum(dim=-1)\n",
    "    indexes = diffs.argmin(axis=1)\n",
    "    # create the grid\n",
    "    image_paths = image_paths[indexes]\n",
    "    indexes = reored_by_left_top(x[indexes])\n",
    "    image_paths = image_paths[indexes]\n",
    "    # create the grid\n",
    "    image = show(\n",
    "        make_grid(\n",
    "            list(map(lambda x: read_image_and_transform(x, size=(128,128)), image_paths)),\n",
    "            nrow=nrow\n",
    "        )\n",
    "    )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcae7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 40 * 60 \n",
    "\n",
    "make_cluster_grid(num_clusters, nrow=60).savefig(dst / f\"rf100-40x60.png\", dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_cluster_grid(num_clusters, nrow=8).savefig(dst / f\"rf100-8x8.png\", dpi=800, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33aa47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
