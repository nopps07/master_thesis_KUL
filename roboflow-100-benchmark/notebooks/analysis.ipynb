{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f917105",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113abe90",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Notebook containing code to create our plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "plt.style.use(['science', 'notebook'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86ca053",
   "metadata": {},
   "source": [
    "## Preambula\n",
    "We are going to run some computations, to save time let's create a decorator that stores and read from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e31589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_load_from_disk(location: Path):\n",
    "    def decorator(func):\n",
    "        def _inner(*args, **kwargs):\n",
    "            if location.exists():\n",
    "                print(f\"[INFO] loading from {location}\")\n",
    "                with open(location, \"rb\") as f:\n",
    "                    return pickle.load(f)\n",
    "            res = func(*args, **kwargs)\n",
    "            with open(location, \"wb\") as f:\n",
    "                print(f\"[INFO] saving to {location}\")\n",
    "                pickle.dump(res, f)\n",
    "            return res\n",
    "        return _inner\n",
    "    return decorator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../metadata/categories.csv\", index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62926f8a",
   "metadata": {},
   "source": [
    "# Sizes\n",
    "\n",
    "Find out total dataset sizes, we have `rf100` download at `rf100`. We can use the index to iterate and get the size of each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838827a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "RF100_ROOT = Path('../rf100')\n",
    "\n",
    "def count_num_files(dataset: str):\n",
    "    dataset_path = RF100_ROOT / dataset\n",
    "    sub_dirs = [\"train\", \"valid\", \"test\"]\n",
    "    num_files = defaultdict(int)\n",
    "    for sub_dir in sub_dirs:\n",
    "        sub_dir_path = dataset_path / sub_dir / 'images'\n",
    "        num_files[sub_dir] += sum([1 if curr.is_file() else 0 for curr in sub_dir_path.iterdir()])\n",
    "    \n",
    "    return pd.Series(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @maybe_load_from_disk(Path('../temp/df.pkl'))\n",
    "def apply_num_files(df):\n",
    "    df[[\"train\", \"test\", \"valid\"]] = df.apply(lambda row: count_num_files(row.name), axis=1)[[\"train\", \"test\", \"valid\"]]\n",
    "    df[\"size\"] = df[\"train\"] +  df[\"test\"] +  df[\"valid\"]\n",
    "    return df\n",
    "\n",
    "df = apply_num_files(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f21a0",
   "metadata": {},
   "source": [
    "We now want to add the number of classes for each dataset, obtained before hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# obtained by running `export ROBOFLOW_API_KEY=.... && python ./scripts/get_labels_names.py` \n",
    "with open(\"../metadata/labels_names.json\", 'r') as f: \n",
    "    labels_names = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_classes_per_dataset(labels_names: Dict) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for item in labels_names:\n",
    "        num_classes = len(item[\"classes\"])\n",
    "        records.append({\n",
    "            \"dataset\" : item[\"name\"],\n",
    "            \"num_classes\": num_classes\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records).set_index(\"dataset\")\n",
    "\n",
    "df = df.join(get_num_classes_per_dataset(labels_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca41aa",
   "metadata": {},
   "source": [
    "Finally, we also want to add the yolov5/7 - glip results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"../results.csv\", index_col=0)\n",
    "df = df.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07434e6",
   "metadata": {},
   "source": [
    "Let's see how many of them there are for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec16012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d794a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"num_datasets\"] = 1\n",
    "aggretations = {\n",
    "    \"train\" : \"sum\", \"test\" : \"sum\", \"valid\" : \"sum\", \"size\" : \"sum\", \"num_classes\" : \"sum\",\n",
    "    \"yolov5\": \"mean\", \"yolov7\": \"mean\",\n",
    "    \"num_datasets\" : \"sum\"           \n",
    "                }\n",
    "grouped_df = df.groupby(\"category\").agg(aggretations).reset_index()\n",
    "grouped_df = grouped_df.sort_values(\"size\")\n",
    "grouped_df[\"perc\"] = grouped_df[\"size\"] / grouped_df[\"size\"].sum()\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da60ea",
   "metadata": {},
   "source": [
    "Now, we want to use the order of the categories to sort our original dataframe, till will make it easier to visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360925ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ordered_categories = pd.DataFrame(index=grouped_df.index, data={\"category\": grouped_df.category})\n",
    "df = df_with_ordered_categories.merge(df.reset_index(\"dataset\"), on=\"category\", how=\"inner\")\n",
    "df = df.set_index(\"dataset\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809a972",
   "metadata": {},
   "source": [
    "Let's store it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54988bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../metadata/datasets_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0543c26",
   "metadata": {},
   "source": [
    "## Bounding boxes stats\n",
    "\n",
    "Cool, so we may also want to plot/show the mean size of bboxes for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654efd1",
   "metadata": {},
   "source": [
    "Let's create something to read all the annotations. We can take advantage of PyTorch Dataloader to use multiple cores and make the computation go brum brum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "IGNORE = -1\n",
    "# all images are resized to 640\n",
    "size = (640, 640)\n",
    "\n",
    "class AnnotationDataset(Dataset):\n",
    "    def __init__(self, root: Path, fmt: str = \"txt\"):\n",
    "        super().__init__()\n",
    "        self.annotations_paths = list(root.glob(f\"**/*.{fmt}\"))\n",
    "    \n",
    "    def maybe_convert_polygon_to_bbox(self, line: str):\n",
    "        splitted = line.split(\" \")\n",
    "        label, rest = splitted[0], splitted[1:]\n",
    "        label = torch.as_tensor(int(label))\n",
    "        is_bbox = len(rest) == 4\n",
    "        if is_bbox:\n",
    "            return  label, torch.as_tensor([float(el) for el in rest])\n",
    "        else:\n",
    "            # must be a polygon\n",
    "            poly = torch.as_tensor([float(el) for el in rest])\n",
    "            poly = poly.view(-1, 2)\n",
    "            xmax, ymax = torch.max(poly, dim=0).values\n",
    "            xmin, ymin = torch.min(poly, dim=0).values\n",
    "            width, heigh = xmax - xmin, ymax - ymin\n",
    "            xcenter, ycenter =  xmin + width / 2, ymin + heigh / 2\n",
    "            return label, torch.stack([xcenter, ycenter, width, heigh])\n",
    "            \n",
    "    def __getitem__(self, idx: int):\n",
    "        with self.annotations_paths[idx].open('r') as f:\n",
    "            for line in f.readlines():\n",
    "                label, bbox = self.maybe_convert_polygon_to_bbox(line)\n",
    "                return label, bbox \n",
    "            return  torch.tensor(IGNORE), torch.as_tensor([IGNORE, IGNORE, IGNORE, IGNORE], dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f71fe7",
   "metadata": {},
   "source": [
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33de9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = AnnotationDataset(RF100_ROOT / df.index[0] / 'test' / 'labels')\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc11209",
   "metadata": {},
   "source": [
    "gg. Now we can use a torch `DataLoader` to speed up stuff. Let's define a couple of functions to help us out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c70374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_areas_and_labels(dataset: str, split: str =\"test\"):\n",
    "    ds = AnnotationDataset(RF100_ROOT / dataset / split / 'labels')\n",
    "    dl = DataLoader(ds, \n",
    "#                     num_workers=1, \n",
    "                    batch_size=128)\n",
    "\n",
    "    all_areas = None\n",
    "    all_labels = None\n",
    "    for (labels, bboxes) in dl:\n",
    "        bboxes = bboxes[labels != IGNORE] \n",
    "        # area = w * h\n",
    "        areas = bboxes[:,2] * bboxes[:,3]\n",
    "        all_areas = torch.cat((all_areas, areas)) if all_areas is not None else areas\n",
    "        all_labels = torch.cat((all_labels, labels)) if all_labels is not None else labels\n",
    "\n",
    "    return all_areas, all_labels\n",
    "\n",
    "\n",
    "def compute_stats(areas: torch.Tensor):\n",
    "    # let's compute the number of small, medium and large bbox\n",
    "    bins = torch.histc(areas, bins=3, min=0, max=0.3)\n",
    "    return areas.mean(), areas.std(), *bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@maybe_load_from_disk(Path(\"../temp/bbox.pkl\"))\n",
    "def create_bbox_df(df):\n",
    "    records = []\n",
    "    dataset_bar = tqdm(df.index)\n",
    "    for dataset in dataset_bar:\n",
    "        dataset_bar.set_postfix_str(dataset)\n",
    "        split_bar = tqdm([\"train\", \"test\", \"valid\"], leave=False)\n",
    "        for split in split_bar:\n",
    "            split_bar.set_postfix_str(split)\n",
    "            areas, labels = get_areas_and_labels(dataset, split)\n",
    "            vals = compute_stats(areas)\n",
    "            vals = [val.float().item() for val in vals]\n",
    "            area_mean, area_std, num_small, num_medium, num_large = vals\n",
    "            labels = labels[labels != IGNORE]\n",
    "            records.append(dict(\n",
    "                                num_classes=labels.unique().numpy().shape[0],\n",
    "                                labels=labels.unique().numpy(),\n",
    "                                areas=areas.numpy(),\n",
    "                                area_mean=area_mean, \n",
    "                                area_std=area_std, \n",
    "                                num_small=num_small, \n",
    "                                num_medium=num_medium, \n",
    "                                num_large=num_large,\n",
    "                                split=split,\n",
    "                                dataset=dataset,\n",
    "                            )\n",
    "                          )\n",
    "\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df = create_bbox_df(df)\n",
    "bbox_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = bbox_df[bbox_df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "valid_df = bbox_df[bbox_df[\"split\"] == \"valid\"].reset_index(drop=True)\n",
    "test_df = bbox_df[bbox_df[\"split\"] == \"test\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7612dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b95209",
   "metadata": {},
   "source": [
    "check if we have all the labels in all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "all_missing_labels = []\n",
    "all_is_correct = []\n",
    "for idx, (train_labels, valid_labels, test_labels) in tqdm(\n",
    "                                zip(train_df.index,\n",
    "                                    zip(train_df[\"labels\"].values,  valid_df[\"labels\"].values,  test_df[\"labels\"].values\n",
    "                                       )\n",
    "                                   ),\n",
    "                                total=len(train_df)):\n",
    "    # see https://numpy.org/doc/stable/reference/generated/numpy.setdiff1d.html\n",
    "    missing_from_valid = np.setdiff1d(valid_labels, train_labels)\n",
    "    missing_from_test = np.setdiff1d(test_labels, train_labels)\n",
    "    missing_labels = np.array([])\n",
    "    \n",
    "    if missing_from_valid.shape[0] > 0:\n",
    "        missing_labels = np.concatenate((missing_labels, missing_from_valid))\n",
    "    if missing_from_test.shape[0] > 0:\n",
    "        missing_labels = np.concatenate((missing_labels, missing_from_test))\n",
    "    \n",
    "    is_correct = missing_labels.shape[0] == 0\n",
    "    all_missing_labels.append(missing_labels)\n",
    "    all_is_correct.append(is_correct)\n",
    "\n",
    "train_df[\"missing_labels\"] = all_missing_labels\n",
    "train_df[\"is_correct\"] = all_is_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65602f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df = train_df[~train_df[\"is_correct\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_df[[\"dataset\", \"missing_labels\"]].to_csv(\"missing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f48c0",
   "metadata": {},
   "source": [
    "Let's add all the prev informations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb6bab",
   "metadata": {},
   "source": [
    "merging areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {}\n",
    "\n",
    "for idx, train_area, valid_area, test_area in zip(train_df[\"dataset\"], \n",
    "                                                  train_df[\"areas\"].values, \n",
    "                                                  valid_df[\"areas\"].values,\n",
    "                                                  test_df[\"areas\"].values):\n",
    "    records[idx] = np.concatenate([train_area, valid_area, test_area])\n",
    "    \n",
    "\n",
    "areas_series = pd.Series(records)\n",
    "areas_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79cf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df_grouped[\"areas\"] = areas_series\n",
    "bbox_df_grouped.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fcc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_bbox = df.join(bbox_df_grouped, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 40))\n",
    "plot = sns.boxplot(data=df_with_bbox[\"areas\"], orient='h')\n",
    "plt.xlabel(\"bbox\")\n",
    "plt.ylabel(\"dataset\")\n",
    "plot.set_yticklabels(df.index)\n",
    "plt.gcf().savefig(\"plot_all_train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e582f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gcf().savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3f3cc",
   "metadata": {},
   "source": [
    "### Clip Embeddings\n",
    "\n",
    "I have sampled 512 points per dataset and encoded them with CLIP. Let's load them, avg them and plot in 2D (after PCA). Let's do it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0acf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from typing import Callable\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RF100_ROOT = Path(\"./rf100/\")\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root: Path, fmt: str = \"jpg\", transform: Callable = None):\n",
    "        super().__init__()\n",
    "        self.images_path = list(root.glob(f\"**/*.{fmt}\"))\n",
    "        self.transform = transform or ToTensor()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = Image.open(self.images_path[idx]).convert(\"RGB\")\n",
    "        return self.transform(image), idx, str(self.images_path[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "def pca(x, k, center=True):\n",
    "    if center:\n",
    "        m = x.mean(0, keepdim=True)\n",
    "        s = x.std(0, unbiased=False, keepdim=True)\n",
    "        x -= m\n",
    "        x /= s\n",
    "    # why pca related to svd? https://www.cs.cmu.edu/~elaw/papers/pca.pdf chap VI\n",
    "    U, S, V = torch.linalg.svd(x) \n",
    "    reduced = torch.mm(x, V[:k].T)\n",
    "\n",
    "    return reduced\n",
    "\n",
    "@maybe_load_from_disk(Path(\"./embeddings_means.pkl\"))\n",
    "def get_embeddings(df):\n",
    "    MAX_BATCHES = 2\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=True)\n",
    "    records = []\n",
    "    for dataset in tqdm(df.index):\n",
    "        ds = ImageDataset(RF100_ROOT / dataset / \"train/images\", transform=preprocess)\n",
    "        dl = DataLoader(\n",
    "            ds, batch_size=256, num_workers=8, pin_memory=True, shuffle=True\n",
    "        )  # we shuffle and we sample MAX_BATCHES batches per dataset\n",
    "        i = 0\n",
    "        means = None\n",
    "        for (x, _, _) in dl:\n",
    "            with torch.no_grad():\n",
    "                x = x.to(\"cuda\")\n",
    "                x = model.encode_image(x)\n",
    "                means = torch.vstack((means, x.mean(0))) if means is not None else x.mean(0)\n",
    "            i += 1\n",
    "            if i >= MAX_BATCHES: break\n",
    "        if len(means.shape) == 1: means = means.unsqueeze(0)\n",
    "        \n",
    "        records.append(dict(dataset=dataset, clip_mean=means.mean(0).squeeze().cpu().numpy()))\n",
    "            \n",
    "    return pd.DataFrame.from_records(records, index=df.index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aeb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = get_embeddings(df)\n",
    "embed_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_means = torch.stack([torch.from_numpy(el) for el in embed_df.clip_mean.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9614baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_means_reduced = pca(clip_means.float(), k=2)\n",
    "clip_means_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = sns.scatterplot(x=clip_means_reduced[:,0], y=clip_means_reduced[:,1], size=df[\"size\"], sizes=(0, 500), hue=df.category, alpha=.66, legend=\"brief\")\n",
    "sns.move_legend(ax, bbox_to_anchor=(1.02, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"embedds.png\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b350266",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"datasets.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2,  figsize=(20,16))\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.titlepad'] = 16\n",
    "# fig.suptitle('Datasets Categories')\n",
    "axs[0,0].set_title(\"Total datasets size/category\")\n",
    "ax = sns.barplot(data=grouped_df, x=\"size\", y=\"category\", linewidth=2,  edgecolor=\".2\", ax=axs[0,0])\n",
    "ax.set_ylabel('category')\n",
    "ax.set_xlabel('size')\n",
    "secax = ax.secondary_xaxis('top', functions=(lambda x: x / df_with_bbox[\"size\"].sum(), lambda x:x))\n",
    "secax.set_xlabel('size (%)')\n",
    "ax.minorticks_on()\n",
    "secax.minorticks_on()\n",
    "\n",
    "axs[0,1].set_title(\"Mean datasets size/category\")\n",
    "ax = sns.boxplot(data=df_with_bbox, x=\"size\", y=\"category\", ax=axs[0,1])\n",
    "ax.set_xlabel('size')\n",
    "ax.get_yaxis().set_visible(False)\n",
    "secax = ax.secondary_xaxis('top', functions=(lambda x: x / df_with_bbox[\"size\"].sum(), lambda x:x))\n",
    "secax.set_xlabel('size (%)')\n",
    "ax.minorticks_on()\n",
    "secax.minorticks_on()\n",
    "\n",
    "axs[1,0].set_title(\"Mean bbox area\")\n",
    "ax = sns.boxplot(data=df_with_bbox, x=\"area_mean\", y=\"category\", ax=axs[1,0])\n",
    "ax.set_xlabel(\"bbox\")\n",
    "\n",
    "axs[1,1].set_title(\"Mean num_classes\")\n",
    "ax = sns.boxplot(data=df_with_bbox, x=\"num_classes\", y=\"category\", ax=axs[1,1])\n",
    "ax.set_xlabel(\"labels\")\n",
    "ax.get_yaxis().set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc6098",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"datasets_stats.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
